#!/usr/bin/env python3
"""
RedBlackBench SFT Training Script

è®­ç»ƒ Qwen3 æ¨¡å‹è¿›è¡Œå›šå¾’å›°å¢ƒåœºæ™¯çš„æˆ˜ç•¥æ¨ç†ã€‚

GPU éœ€æ±‚ä¼°ç®— (Qwen3-14B, LoRA r=64):
- æ¨¡å‹æƒé‡ (BF16): ~28GB
- æ¢¯åº¦: ~28GB  
- ä¼˜åŒ–å™¨çŠ¶æ€: ~0.4GB (LoRA only)
- æ¿€æ´»å€¼ (with checkpointing): ~10GB
- æ€»è®¡: ~67GB

æ¨èé…ç½®:
- A100 (80GB): 1å¼ è¶³å¤Ÿï¼Œå¯ç”¨ batch_size=2
- H100 (96GB): 1å¼ è¶³å¤Ÿï¼Œå¯ç”¨ batch_size=4
- H200 (192GB): 1å¼ ï¼Œå¯ç”¨æ›´å¤§ batch_size

å¯¹äº Qwen3-8B (~16GB æƒé‡):
- A100 (80GB): 1å¼ ï¼Œbatch_size=4
- H100 (96GB): 1å¼ ï¼Œbatch_size=8
"""

import os
import sys
import json
import argparse
import math
import torch
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Optional, Dict

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
)
from peft import LoraConfig, get_peft_model, TaskType
from torch.utils.data import Dataset

# æ·»åŠ  redblackbench åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    # Model
    model_name: str = "Qwen/Qwen3-8B"
    
    # LoRA
    lora_r: int = 64
    lora_alpha: int = 128
    lora_dropout: float = 0.05
    target_modules: List[str] = field(default_factory=lambda: [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ])
    
    # Data
    train_path: str = "data/train.jsonl"
    val_path: str = "data/val.jsonl"
    max_length: int = 4096
    
    # Training
    output_dir: str = "outputs"
    epochs: int = 3
    batch_size: int = 1
    grad_accum: int = 16
    lr: float = 2e-5
    warmup_ratio: float = 0.1
    weight_decay: float = 0.01
    max_grad_norm: float = 1.0
    
    # Logging
    logging_steps: int = 10
    save_steps: int = 200
    eval_steps: int = 100
    save_total_limit: int = 3
    
    # Optimization
    bf16: bool = True
    gradient_checkpointing: bool = True
    
    # Early stopping
    patience: int = 5
    threshold: float = 0.001


class RedBlackDataset(Dataset):
    """RedBlackBench æ•°æ®é›†"""
    
    def __init__(self, path: str, tokenizer, max_length: int = 4096):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = []
        
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    self.data.append(json.loads(line))
        
        print(f"  Loaded {len(self.data):,} samples from {path}")
        self._show_length_stats()
    
    def _show_length_stats(self):
        """æ˜¾ç¤º token é•¿åº¦ç»Ÿè®¡"""
        sample_size = min(100, len(self.data))
        lengths = []
        
        for item in self.data[:sample_size]:
            text = self.tokenizer.apply_chat_template(
                item['messages'], tokenize=False, add_generation_prompt=False
            )
            lengths.append(len(self.tokenizer.encode(text)))
        
        if lengths:
            over_limit = sum(1 for l in lengths if l > self.max_length)
            print(f"    Token lengths: min={min(lengths)}, max={max(lengths)}, avg={sum(lengths)/len(lengths):.0f}")
            if over_limit > 0:
                print(f"    âš ï¸  {over_limit}/{sample_size} samples exceed max_length={self.max_length}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        text = self.tokenizer.apply_chat_template(
            item['messages'], tokenize=False, add_generation_prompt=False
        )
        
        encoded = self.tokenizer(
            text,
            max_length=self.max_length,
            truncation=True,
            padding=False,
            return_tensors=None
        )
        
        return {
            'input_ids': encoded['input_ids'],
            'attention_mask': encoded['attention_mask'],
            'labels': encoded['input_ids'].copy()
        }


def collate_fn(batch, pad_token_id: int):
    """åŠ¨æ€å¡«å……"""
    max_len = max(len(x['input_ids']) for x in batch)
    
    input_ids = []
    attention_mask = []
    labels = []
    
    for x in batch:
        pad_len = max_len - len(x['input_ids'])
        input_ids.append(x['input_ids'] + [pad_token_id] * pad_len)
        attention_mask.append(x['attention_mask'] + [0] * pad_len)
        labels.append(x['labels'] + [-100] * pad_len)
    
    return {
        'input_ids': torch.tensor(input_ids),
        'attention_mask': torch.tensor(attention_mask),
        'labels': torch.tensor(labels)
    }


def estimate_gpu_memory(model_name: str, lora_r: int, max_length: int) -> Dict:
    """
    ä¼°ç®— GPU æ˜¾å­˜éœ€æ±‚
    
    åŸºäºç»éªŒå€¼:
    - Qwen3-8B: ~16GB æ¨¡å‹æƒé‡ (BF16), è®­ç»ƒæ—¶å³°å€¼ ~40GB
    - Qwen3-14B: ~28GB æ¨¡å‹æƒé‡ (BF16), è®­ç»ƒæ—¶å³°å€¼ ~60GB
    - Qwen3-32B: ~64GB æ¨¡å‹æƒé‡ (BF16), è®­ç»ƒæ—¶å³°å€¼ ~100GB
    """
    # æ¨¡å‹é…ç½® (å‚æ•°é‡, hidden_dim, num_layers)
    model_configs = {
        "8B": (8e9, 4096, 32),
        "14B": (14e9, 5120, 40),
        "32B": (32e9, 6144, 64),
        "72B": (72e9, 8192, 80),
    }
    
    # ä»æ¨¡å‹åæå–å¤§å°
    params, hidden_dim, num_layers = 8e9, 4096, 32
    for size_str, config in model_configs.items():
        if size_str in model_name:
            params, hidden_dim, num_layers = config
            break
    
    bytes_per_param = 2  # BF16
    
    # 1. æ¨¡å‹æƒé‡ (frozen, åªå ç”¨æ¨ç†å†…å­˜)
    model_mem = params * bytes_per_param
    
    # 2. LoRA å‚æ•° + æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€
    # æ¯å±‚: q,k,v,o,gate,up,down = 7ä¸ªç›®æ ‡æ¨¡å—
    # æ¯ä¸ªæ¨¡å—: 2ä¸ªçŸ©é˜µ (A: hidden_dim x r, B: r x hidden_dim)
    lora_params_per_layer = 7 * 2 * (hidden_dim * lora_r)
    total_lora_params = lora_params_per_layer * num_layers
    # å‚æ•°(BF16) + æ¢¯åº¦(BF16) + AdamçŠ¶æ€(FP32 må’Œv)
    lora_mem = total_lora_params * (2 + 2 + 8)
    
    # 3. æ¿€æ´»å€¼ (with gradient checkpointing)
    # ç²—ç•¥ä¼°ç®—: batch_size=1, seq_len=max_length
    # æ¯å±‚ä¿å­˜: attention scores + ä¸­é—´æ¿€æ´»
    activation_per_layer = max_length * hidden_dim * 4 * bytes_per_param
    activation_mem = activation_per_layer * (num_layers // 4)  # checkpointing å‡å°‘åˆ° 1/4
    
    # 4. é¢å¤–å¼€é”€ (CUDA kernels, workspace, fragmentationç­‰)
    # å®é™…è®­ç»ƒä¸­é€šå¸¸éœ€è¦ 1.5-2x çš„ç†è®ºå€¼
    overhead = 4 * (1024**3)  # ~4GB base overhead
    
    theoretical = model_mem + lora_mem + activation_mem + overhead
    # æ·»åŠ å®‰å…¨è¾¹é™… (1.5x) å› ä¸º PyTorch/CUDA å†…å­˜ç¢ç‰‡åŒ–
    practical = theoretical * 1.5
    practical_gb = practical / (1024**3)
    
    return {
        "model_params": f"{params/1e9:.0f}B",
        "model_memory_gb": model_mem / (1024**3),
        "lora_memory_gb": lora_mem / (1024**3),
        "activation_memory_gb": activation_mem / (1024**3),
        "total_estimated_gb": practical_gb,
        "recommended_gpu": (
            "A100 80GB x1" if practical_gb < 70 else
            "H100 96GB x1" if practical_gb < 85 else
            "H200 192GB x1" if practical_gb < 170 else
            "Multi-GPU required"
        )
    }


def setup_model(config: TrainingConfig):
    """åŠ è½½å¹¶é…ç½®æ¨¡å‹"""
    print("\nğŸ”§ Setting up model...")
    print(f"  Model: {config.model_name}")
    
    # æ£€æŸ¥å¯ç”¨ GPU
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            total_mem = props.total_memory / (1024**3)
            print(f"  GPU {i}: {props.name} ({total_mem:.0f} GB)")
    
    # åŠ è½½æ¨¡å‹
    print("\n  Loading model weights...")
    model = AutoModelForCausalLM.from_pretrained(
        config.model_name,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
    
    # å¯ç”¨ gradient checkpointing
    if config.gradient_checkpointing:
        model.gradient_checkpointing_enable(
            gradient_checkpointing_kwargs={"use_reentrant": False}
        )
    
    # é…ç½® LoRA
    print(f"\n  Configuring LoRA: r={config.lora_r}, alpha={config.lora_alpha}")
    lora_config = LoraConfig(
        r=config.lora_r,
        lora_alpha=config.lora_alpha,
        lora_dropout=config.lora_dropout,
        target_modules=config.target_modules,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    model.config.use_cache = False
    model.enable_input_require_grads()
    
    return model


def estimate_training_time(
    num_samples: int,
    epochs: int,
    batch_size: int,
    grad_accum: int,
    gpu_type: str = "A100"
) -> Dict:
    """ä¼°ç®—è®­ç»ƒæ—¶é—´"""
    total_steps = math.ceil(num_samples / (batch_size * grad_accum)) * epochs
    
    # æ¯æ­¥æ—¶é—´ä¼°ç®— (ç§’)
    step_times = {
        "A100": 4.0,
        "H100": 2.5,
        "H200": 1.5,
    }
    
    step_time = step_times.get(gpu_type, 4.0)
    total_seconds = total_steps * step_time
    
    return {
        "total_steps": total_steps,
        "step_time_seconds": step_time,
        "total_hours": total_seconds / 3600,
        "gpu_type": gpu_type,
    }


def main():
    parser = argparse.ArgumentParser(description="Train RedBlackBench model")
    
    # Model
    parser.add_argument("--model", type=str, default="Qwen/Qwen3-8B")
    parser.add_argument("--lora_r", type=int, default=64)
    parser.add_argument("--lora_alpha", type=int, default=None)  # é»˜è®¤ 2*r
    
    # Data
    parser.add_argument("--train", type=str, default="data/train.jsonl")
    parser.add_argument("--val", type=str, default="data/val.jsonl")
    parser.add_argument("--max_length", type=int, default=4096)
    
    # Training
    parser.add_argument("--output", type=str, default="outputs")
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--batch_size", type=int, default=1)
    parser.add_argument("--grad_accum", type=int, default=16)
    parser.add_argument("--lr", type=float, default=2e-5)
    
    # Logging
    parser.add_argument("--wandb", type=str, default=None, help="WandB project name")
    parser.add_argument("--run_name", type=str, default=None)
    
    # Other
    parser.add_argument("--resume", type=str, default=None)
    parser.add_argument("--estimate_only", action="store_true", help="Only estimate, don't train")
    
    args = parser.parse_args()
    
    # æ„å»ºé…ç½®
    config = TrainingConfig(
        model_name=args.model,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha or args.lora_r * 2,
        train_path=args.train,
        val_path=args.val,
        max_length=args.max_length,
        output_dir=args.output,
        epochs=args.epochs,
        batch_size=args.batch_size,
        grad_accum=args.grad_accum,
        lr=args.lr,
    )
    
    # æ‰“å°é…ç½®
    print("=" * 70)
    print("ğŸ® RedBlackBench SFT Training")
    print("=" * 70)
    print(f"\nğŸ“‹ Configuration:")
    print(f"  Model: {config.model_name}")
    print(f"  LoRA: r={config.lora_r}, alpha={config.lora_alpha}")
    print(f"  Learning rate: {config.lr}")
    print(f"  Epochs: {config.epochs}")
    print(f"  Batch: {config.batch_size} x {config.grad_accum} = {config.batch_size * config.grad_accum}")
    print(f"  Max length: {config.max_length}")
    print(f"  Output: {config.output_dir}")
    
    # GPU æ˜¾å­˜ä¼°ç®— (ä¸éœ€è¦åŠ è½½æ¨¡å‹)
    mem_est = estimate_gpu_memory(config.model_name, config.lora_r, config.max_length)
    print(f"\nğŸ“Š GPU Memory Estimation:")
    print(f"  Model ({mem_est['model_params']}): {mem_est['model_memory_gb']:.1f} GB")
    print(f"  LoRA (r={config.lora_r}): {mem_est['lora_memory_gb']:.1f} GB")
    print(f"  Activations: {mem_est['activation_memory_gb']:.1f} GB")
    print(f"  Total: ~{mem_est['total_estimated_gb']:.0f} GB")
    print(f"  Recommended: {mem_est['recommended_gpu']}")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(config.train_path):
        print(f"\nâŒ Training data not found: {config.train_path}")
        print("   Run prepare_data.py first!")
        return
    
    # ä¼°ç®—è®­ç»ƒæ—¶é—´ (åŸºäºæ–‡ä»¶è¡Œæ•°)
    with open(config.train_path) as f:
        num_train_samples = sum(1 for _ in f)
    
    print(f"\nğŸ“ˆ Training Data: {num_train_samples:,} samples")
    
    for gpu_type in ["A100", "H100", "H200"]:
        est = estimate_training_time(
            num_train_samples, config.epochs, 
            config.batch_size, config.grad_accum, gpu_type
        )
        print(f"\nâ±ï¸  Training time estimate ({gpu_type}):")
        print(f"    Steps: {est['total_steps']:,}")
        print(f"    Time: ~{est['total_hours']:.1f} hours")
    
    if args.estimate_only:
        print("\nâœ“ Estimation complete (--estimate_only)")
        return
    
    # åŠ è½½ tokenizer
    print("\nğŸ“¥ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“¥ Loading datasets...")
    train_dataset = RedBlackDataset(config.train_path, tokenizer, config.max_length)
    val_dataset = RedBlackDataset(config.val_path, tokenizer, config.max_length)
    
    # è®¾ç½®æ¨¡å‹
    model = setup_model(config)
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=config.output_dir,
        num_train_epochs=config.epochs,
        per_device_train_batch_size=config.batch_size,
        per_device_eval_batch_size=config.batch_size,
        gradient_accumulation_steps=config.grad_accum,
        learning_rate=config.lr,
        warmup_ratio=config.warmup_ratio,
        weight_decay=config.weight_decay,
        max_grad_norm=config.max_grad_norm,
        logging_steps=config.logging_steps,
        save_steps=config.save_steps,
        eval_strategy="steps",
        eval_steps=config.eval_steps,
        save_total_limit=config.save_total_limit,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        bf16=config.bf16,
        gradient_checkpointing=False,  # å·²åœ¨æ¨¡å‹ä¸Šå¯ç”¨
        dataloader_num_workers=0,
        report_to="wandb" if args.wandb else "none",
        run_name=args.run_name or f"redblack-r{config.lora_r}-lr{config.lr}",
        remove_unused_columns=False,
        logging_first_step=True,
    )
    
    # Callbacks
    callbacks = [
        EarlyStoppingCallback(
            early_stopping_patience=config.patience,
            early_stopping_threshold=config.threshold,
        )
    ]
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=lambda b: collate_fn(b, tokenizer.pad_token_id),
        callbacks=callbacks,
    )
    
    # è®­ç»ƒ
    print("\n" + "=" * 70)
    print("ğŸš€ Starting Training")
    print("=" * 70)
    
    if args.resume:
        print(f"Resuming from: {args.resume}")
        trainer.train(resume_from_checkpoint=args.resume)
    else:
        trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    final_path = os.path.join(config.output_dir, "final_model")
    print(f"\nğŸ’¾ Saving final model to: {final_path}")
    trainer.save_model(final_path)
    tokenizer.save_pretrained(final_path)
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config_dict = {
        "model_name": config.model_name,
        "lora_r": config.lora_r,
        "lora_alpha": config.lora_alpha,
        "lora_dropout": config.lora_dropout,
        "learning_rate": config.lr,
        "epochs": config.epochs,
        "max_length": config.max_length,
        "batch_size": config.batch_size,
        "grad_accum": config.grad_accum,
        "train_samples": len(train_dataset),
        "val_samples": len(val_dataset),
    }
    with open(os.path.join(final_path, "training_config.json"), 'w') as f:
        json.dump(config_dict, f, indent=2)
    
    print("\n" + "=" * 70)
    print("âœ… Training Complete!")
    print(f"   Model saved to: {final_path}")
    print("=" * 70)


if __name__ == "__main__":
    main()

