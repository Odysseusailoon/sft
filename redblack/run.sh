#!/bin/bash
# RedBlackBench SFT ËÆ≠ÁªÉËÑöÊú¨
# 
# ‰ΩøÁî®ÊñπÊ≥ï:
#   ./run.sh prepare           # ÂáÜÂ§áÊï∞ÊçÆ
#   ./run.sh train             # ËÆ≠ÁªÉ (ÈªòËÆ§ 8B)
#   ./run.sh train --model Qwen/Qwen3-14B  # ËÆ≠ÁªÉ 14B
#   ./run.sh estimate          # Âè™‰º∞ÁÆóËµÑÊ∫êÈúÄÊ±Ç

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

# ÈªòËÆ§ÂèÇÊï∞
MODEL="${MODEL:-Qwen/Qwen3-8B}"
EPOCHS="${EPOCHS:-3}"
LORA_R="${LORA_R:-64}"
LR="${LR:-2e-5}"
BATCH_SIZE="${BATCH_SIZE:-1}"
GRAD_ACCUM="${GRAD_ACCUM:-16}"
MAX_LENGTH="${MAX_LENGTH:-4096}"
OUTPUT_DIR="${OUTPUT_DIR:-outputs}"

show_help() {
    echo "Usage: $0 <command> [options]"
    echo ""
    echo "Commands:"
    echo "  prepare     Prepare training data from HuggingFace dataset"
    echo "  train       Train the model"
    echo "  estimate    Estimate GPU requirements without training"
    echo ""
    echo "Environment Variables:"
    echo "  MODEL       Model name (default: Qwen/Qwen3-8B)"
    echo "  EPOCHS      Number of epochs (default: 3)"
    echo "  LORA_R      LoRA rank (default: 64)"
    echo "  LR          Learning rate (default: 2e-5)"
    echo "  BATCH_SIZE  Per-device batch size (default: 1)"
    echo "  GRAD_ACCUM  Gradient accumulation steps (default: 16)"
    echo "  OUTPUT_DIR  Output directory (default: outputs)"
    echo ""
    echo "Examples:"
    echo "  $0 prepare"
    echo "  $0 train"
    echo "  MODEL=Qwen/Qwen3-14B EPOCHS=5 $0 train"
    echo "  $0 estimate --model Qwen/Qwen3-14B"
}

prepare_data() {
    echo "=========================================="
    echo "üìä Preparing RedBlackBench SFT Data"
    echo "=========================================="
    
    python prepare_data.py \
        --data_dir hf_dataset \
        --output_dir data \
        --save_by_scenario \
        "$@"
}

train_model() {
    echo "=========================================="
    echo "üöÄ Training RedBlackBench Model"
    echo "=========================================="
    echo "Model: $MODEL"
    echo "Epochs: $EPOCHS"
    echo "LoRA r: $LORA_R"
    echo "LR: $LR"
    echo "Batch: $BATCH_SIZE x $GRAD_ACCUM"
    echo "=========================================="
    
    python train.py \
        --model "$MODEL" \
        --epochs "$EPOCHS" \
        --lora_r "$LORA_R" \
        --lr "$LR" \
        --batch_size "$BATCH_SIZE" \
        --grad_accum "$GRAD_ACCUM" \
        --max_length "$MAX_LENGTH" \
        --output "$OUTPUT_DIR" \
        "$@"
}

estimate_resources() {
    echo "=========================================="
    echo "üìä Estimating GPU Requirements"
    echo "=========================================="
    
    python train.py \
        --model "$MODEL" \
        --epochs "$EPOCHS" \
        --lora_r "$LORA_R" \
        --estimate_only \
        "$@"
}

# ‰∏ªÂÖ•Âè£
case "${1:-help}" in
    prepare)
        shift
        prepare_data "$@"
        ;;
    train)
        shift
        train_model "$@"
        ;;
    estimate)
        shift
        estimate_resources "$@"
        ;;
    help|--help|-h)
        show_help
        ;;
    *)
        echo "Unknown command: $1"
        show_help
        exit 1
        ;;
esac

